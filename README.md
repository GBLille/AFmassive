![header](imgs/header.jpg)

# MassiveFold

This AlphaFold version aims at massively expand the sampling of structure predictions following Björn Wallner's AFsample 
version of AlphaFold (https://github.com/bjornwallner/alphafoldv2.2.0/)
and to provide some optimizations in the computing.
These optimizations are described below with the flags that were added to the genuine DeepMind's AlphaFold.

It was started with a fork of the DeepMind's AlphaFold v2.3.1 - 10/03/2023: https://github.com/deepmind/alphafold

# Setup
The setup is the same as the one for AlphaFold v2.3.1. However, v1 and v2 neural network (NN) model parameters have to be present in the
*param* folder and should contain the version number in the name. Therefore, the list of NN 
model parameters in the folder should be as follows:

params_model_1_multimer_v1.npz  
params_model_1_multimer_v2.npz  
params_model_1_multimer_v3.npz  
params_model_1.npz  
params_model_1_ptm.npz  
params_model_2_multimer_v1.npz  
params_model_2_multimer_v2.npz  
params_model_2_multimer_v3.npz  
params_model_2.npz  
params_model_2_ptm.npz  
params_model_3_multimer_v1.npz  
params_model_3_multimer_v2.npz  
params_model_3_multimer_v3.npz  
params_model_3.npz  
params_model_3_ptm.npz  
params_model_4_multimer_v1.npz  
params_model_4_multimer_v2.npz  
params_model_4_multimer_v3.npz  
params_model_4.npz  
params_model_4_ptm.npz  
params_model_5_multimer_v1.npz  
params_model_5_multimer_v2.npz  
params_model_5_multimer_v3.npz  
params_model_5.npz  
params_model_5_ptm.npz  

# Added flags
Here is the list of flags added to AlphaFold 2.3.1 (the intermediate version of AlphaFold includes the of 2.3.2) and their description, also accessible through the --help
option.

  **--alignments_only**: Whether to generate only alignments. Only alignments will be generated by the data pipeline, the modelling will not be performed
    (default: 'false')  
  **--bfd_max_hits**: Max hits in BFD/uniref MSA
    (default: '10000')
    (an integer)  
  **--dropout**: Turn on drop out during inference to get more diversity
    (default: 'false')  
  **--dropout_rates_filename**: Provide dropout rates for inference from a json file. If None, default rates are used, if "dropout" is True.
  **--early_stop_tolerance**: Early stopping threshold for recycling
    (default: '0.5')
    (a number)  
  **--max_recycles**: Maximum number of recycles to run
    (default: '20')
    (an integer)  
  **--mgnify_max_hits**: Max hits in mgnify MSA
    (default: '500')
    (an integer)  
  **--model_preset**: <monomer|monomer_casp14|monomer_ptm|multimer|multimer_v1|multimer_v2|multimer_v3>: Choose preset model configuration - the monomer model, the monomer model with extra ensembling, monomer model with pTM head, or
    multimer model; "multimer" computes the 3 versions of multimer models
    (default: 'monomer')  
  **--models_to_use**: specify which models in model_preset that should be run (cf list in "Setup" section)
    (a comma separated list)  
  **--no_templates**: will not use any template, will be faster than filter by date
    (default: 'false')  
  **--start_prediction**: model to start with, can be used to parallelize jobs,  
  e.g --num_predictions_per_model 20 --start_prediction 20 will only make model _20  
  e.g --num_predictions_per_model 21 --start_prediction 20 will make model _20 and _21 etc.
    (default: '1')
    (an integer)  
  **--template_mmcif_dir**: Path to a directory with template mmCIF structures, each named <pdb_id>.cif  
  **--uniprot_max_hits**: Max hits in uniprot MSA
    (default: '50000')
    (an integer)  
  **--uniref_max_hits**: Max hits in uniref MSA
    (default: '10000')
    (an integer)  

# Authors
Guillaume Brysbaert (UGSF - UMR 8576, France)  
Nessim Raouraoua (UGSF - UMR 8576, France)  
Christophe Blanchet (IFB, France)  
Claudio Mirabello (NBIS, Sweden)  
Björn Wallner (Linköping University, Sweden)  

This work was completed in part at the IDRIS Open Hackathon (http://www.idris.fr/annonces/idris-gpu-hackathon-2023.html), part of the Open Hackathons program. The authors would like to acknowledge OpenACC-Standard.org for their support.